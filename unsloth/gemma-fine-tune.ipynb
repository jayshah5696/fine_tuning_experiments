{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\nif major_version >= 8:\n    # Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100, H100, L40)\n    !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\nelse:\n    # Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n    !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\npass","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-03T02:34:37.516695Z","iopub.execute_input":"2024-03-03T02:34:37.517019Z","iopub.status.idle":"2024-03-03T02:38:41.480947Z","shell.execute_reply.started":"2024-03-03T02:34:37.516989Z","shell.execute_reply":"2024-03-03T02:38:41.479796Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install watermark ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:38:41.482963Z","iopub.execute_input":"2024-03-03T02:38:41.483524Z","iopub.status.idle":"2024-03-03T02:38:54.770278Z","shell.execute_reply.started":"2024-03-03T02:38:41.483497Z","shell.execute_reply":"2024-03-03T02:38:54.769318Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting watermark\n  Downloading watermark-2.4.3-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: ipython>=6.0 in /opt/conda/lib/python3.10/site-packages (from watermark) (8.20.0)\nRequirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.10/site-packages (from watermark) (6.11.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from watermark) (69.0.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (5.9.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=6.0->watermark) (4.8.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.0->watermark) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.0->watermark) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.0->watermark) (1.16.0)\nDownloading watermark-2.4.3-py2.py3-none-any.whl (7.6 kB)\nInstalling collected packages: watermark\nSuccessfully installed watermark-2.4.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:38:54.771621Z","iopub.execute_input":"2024-03-03T02:38:54.771897Z","iopub.status.idle":"2024-03-03T02:39:24.279134Z","shell.execute_reply.started":"2024-03-03T02:38:54.771871Z","shell.execute_reply":"2024-03-03T02:39:24.277853Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nCollecting pyarrow>=12.0.0 (from datasets)\n  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\nCollecting pyarrow-hotfix (from datasets)\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nInstalling collected packages: pyarrow-hotfix, pyarrow, datasets\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 11.0.0\n    Uninstalling pyarrow-11.0.0:\n      Successfully uninstalled pyarrow-11.0.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.18.0 pyarrow-15.0.0 pyarrow-hotfix-0.6\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install -U flash-attn","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:39:24.281829Z","iopub.execute_input":"2024-03-03T02:39:24.282183Z","iopub.status.idle":"2024-03-03T02:39:24.288835Z","shell.execute_reply.started":"2024-03-03T02:39:24.282153Z","shell.execute_reply":"2024-03-03T02:39:24.287901Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nmajor_version, minor_version = torch.cuda.get_device_capability()\nprint(major_version,minor_version)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:39:24.289905Z","iopub.execute_input":"2024-03-03T02:39:24.290228Z","iopub.status.idle":"2024-03-03T02:39:24.300149Z","shell.execute_reply.started":"2024-03-03T02:39:24.290204Z","shell.execute_reply":"2024-03-03T02:39:24.299229Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"7 5\n","output_type":"stream"}]},{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n    print('Not connected to a GPU')\nelse:\n    print(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:39:24.301340Z","iopub.execute_input":"2024-03-03T02:39:24.301658Z","iopub.status.idle":"2024-03-03T02:39:24.417761Z","shell.execute_reply.started":"2024-03-03T02:39:24.301636Z","shell.execute_reply":"2024-03-03T02:39:24.416856Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Sun Mar  3 02:39:24 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   33C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n|   1  Tesla T4                       Off | 00000000:00:05.0 Off |                    0 |\n| N/A   33C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install watermark\n\n%load_ext watermark\n%watermark --conda -p torch,transformers,accelerate,bitsandbytes,unsloth","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:39:24.418833Z","iopub.execute_input":"2024-03-03T02:39:24.419108Z","iopub.status.idle":"2024-03-03T02:39:30.316068Z","shell.execute_reply.started":"2024-03-03T02:39:24.419084Z","shell.execute_reply":"2024-03-03T02:39:30.315067Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/unsloth/__init__.py:71: UserWarning: Unsloth: Running `ldconfig /usr/lib64-nvidia` to link CUDA.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"torch       : 2.1.0\ntransformers: 4.38.1\naccelerate  : 0.27.2\nbitsandbytes: 0.42.0\nunsloth     : 2024.2\n\nconda environment: n/a\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-bnb-4bit\",\n    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n    \"unsloth/llama-2-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-bnb-4bit\",\n    \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b\n    \"unsloth/gemma-2b-bnb-4bit\",\n    \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b\n] # More models at https://huggingface.co/unsloth\nmodel_name = \"unsloth/gemma-7b-bnb-4bit\"\n# model_name = \"unsloth/gemma-7b\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_name, # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:39:30.317242Z","iopub.execute_input":"2024-03-03T02:39:30.317658Z","iopub.status.idle":"2024-03-03T02:40:50.355470Z","shell.execute_reply.started":"2024-03-03T02:39:30.317632Z","shell.execute_reply":"2024-03-03T02:40:50.354660Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5bd9060a3e461b92d3c1cc7186c249"}},"metadata":{}},{"name":"stdout","text":"==((====))==  Unsloth: Fast Gemma patching release 2024.2\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.1.2. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. Xformers = 0.0.22.post7. FA = False.\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9454d1924a5476fb8644520a656687f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af583c86077d4d039d248ec39a1eea57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d55e7cb8696948c5afa33e464967764b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4569f58fcef94e30a2bee4f2dff82fde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5876670c1d8d47f4813ae8f9ae120788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"127ff5df754649b1aa4c72a7cc544813"}},"metadata":{}},{"name":"stderr","text":"2024-03-03 02:40:40.607998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-03 02:40:40.608116: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-03 02:40:40.843102: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 32,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    use_gradient_checkpointing = True,\n    random_state = 112,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:40:50.356839Z","iopub.execute_input":"2024-03-03T02:40:50.357576Z","iopub.status.idle":"2024-03-03T02:40:51.299956Z","shell.execute_reply.started":"2024-03-03T02:40:50.357538Z","shell.execute_reply":"2024-03-03T02:40:51.299171Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Unsloth 2024.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\ndef formatting_prompts_func(examples):\n#     instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    texts = []\n    for input_, output in zip( inputs, outputs):\n        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n        text = alpaca_prompt.format( input_, output) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\npass\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:40:51.303181Z","iopub.execute_input":"2024-03-03T02:40:51.303492Z","iopub.status.idle":"2024-03-03T02:40:51.310016Z","shell.execute_reply.started":"2024-03-03T02:40:51.303466Z","shell.execute_reply":"2024-03-03T02:40:51.308910Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"jayshah5696/indic_guj_hi_en_mix\", split = \"train\")\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:40:51.311188Z","iopub.execute_input":"2024-03-03T02:40:51.311496Z","iopub.status.idle":"2024-03-03T02:41:21.292281Z","shell.execute_reply.started":"2024-03-03T02:40:51.311443Z","shell.execute_reply":"2024-03-03T02:41:21.291310Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/896 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcb36a06e3147ae9d517bf476e38f6b"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 153M/153M [00:08<00:00, 18.8MB/s] \nDownloading data: 100%|██████████| 107M/107M [00:05<00:00, 19.2MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/524222 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed5489957bbb44759385154c8e679a9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/524222 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e6ddb7aa7c4402bd72e1f8c16aa165"}},"metadata":{}}]},{"cell_type":"code","source":"shuffled_data = dataset.shuffle(seed=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:41:21.293712Z","iopub.execute_input":"2024-03-03T02:41:21.294035Z","iopub.status.idle":"2024-03-03T02:41:22.245316Z","shell.execute_reply.started":"2024-03-03T02:41:21.294007Z","shell.execute_reply":"2024-03-03T02:41:22.244316Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = shuffled_data,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = True, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 112,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:41:22.246590Z","iopub.execute_input":"2024-03-03T02:41:22.246888Z","iopub.status.idle":"2024-03-03T02:45:34.978999Z","shell.execute_reply.started":"2024-03-03T02:41:22.246864Z","shell.execute_reply":"2024-03-03T02:45:34.978114Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b62adaad3c8a44b6a3142538c637041b"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (9159 > 8192). Running this sequence through the model will result in indexing errors\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"#@title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:45:34.980076Z","iopub.execute_input":"2024-03-03T02:45:34.980371Z","iopub.status.idle":"2024-03-03T02:45:34.987152Z","shell.execute_reply.started":"2024-03-03T02:45:34.980346Z","shell.execute_reply":"2024-03-03T02:45:34.986189Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.748 GB.\n6.141 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:45:34.988386Z","iopub.execute_input":"2024-03-03T02:45:34.988667Z","iopub.status.idle":"2024-03-03T02:47:06.732966Z","shell.execute_reply.started":"2024-03-03T02:45:34.988645Z","shell.execute_reply":"2024-03-03T02:47:06.730931Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 28,140 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 16 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 50,003,968\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240303_024628-c4rqitbg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jayshah5696/huggingface/runs/c4rqitbg' target=\"_blank\">iconic-silence-8</a></strong> to <a href='https://wandb.ai/jayshah5696/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jayshah5696/huggingface' target=\"_blank\">https://wandb.ai/jayshah5696/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jayshah5696/huggingface/runs/c4rqitbg' target=\"_blank\">https://wandb.ai/jayshah5696/huggingface/runs/c4rqitbg</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<string>:354\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:185\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    184\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 185\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:200\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:110\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    108\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 110\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mNotImplementedError\u001b[0m: Caught NotImplementedError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 775, in PeftModelForCausalLM_fast_forward\n    return self.base_model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 160, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 710, in LlamaForCausalLM_fast_forward\n    outputs = self.model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 582, in LlamaModel_fast_forward\n    layer_outputs = torch.utils.checkpoint.checkpoint(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_compile.py\", line 24, in inner\n    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 451, in checkpoint\n    return CheckpointFunction.apply(function, preserve, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 539, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 230, in forward\n    outputs = run_function(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 578, in custom_forward\n    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/gemma.py\", line 97, in GemmaDecoderLayer_fast_forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 318, in LlamaAttention_fast_forward\n    A = xformers_attention(Q, K, V, attn_bias = causal_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 223, in memory_efficient_attention\n    return _memory_efficient_attention(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 321, in _memory_efficient_attention\n    return _memory_efficient_attention_forward(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 337, in _memory_efficient_attention_forward\n    op = _dispatch_fw(inp, False)\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py\", line 120, in _dispatch_fw\n    return _run_priority_list(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py\", line 63, in _run_priority_list\n    raise NotImplementedError(msg)\nNotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(2, 4096, 16, 256) (torch.float16)\n     key         : shape=(2, 4096, 16, 256) (torch.float16)\n     value       : shape=(2, 4096, 16, 256) (torch.float16)\n     attn_bias   : <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n     p           : 0.0\n`decoderF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 128\n    xFormers wasn't build with CUDA support\n    attn_bias type is <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n    operator wasn't built - see `python -m xformers.info` for more info\n`flshattF@0.0.0` is not supported because:\n    xFormers wasn't build with CUDA support\n    requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)\n    operator wasn't built - see `python -m xformers.info` for more info\n`tritonflashattF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 128\n    xFormers wasn't build with CUDA support\n    requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)\n    operator wasn't built - see `python -m xformers.info` for more info\n    triton is not available\n    requires GPU with sm80 minimum compute capacity, e.g., A100/H100/L4\n    Only work on pre-MLIR triton for now\n`cutlassF` is not supported because:\n    xFormers wasn't build with CUDA support\n    operator wasn't built - see `python -m xformers.info` for more info\n`smallkF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 32\n    xFormers wasn't build with CUDA support\n    dtype=torch.float16 (supported: {torch.float32})\n    attn_bias type is <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n    operator wasn't built - see `python -m xformers.info` for more info\n    unsupported embed per head: 256\n"],"ename":"NotImplementedError","evalue":"Caught NotImplementedError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py\", line 85, in _worker\n    output = module(*input, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 775, in PeftModelForCausalLM_fast_forward\n    return self.base_model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 160, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 710, in LlamaForCausalLM_fast_forward\n    outputs = self.model(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 582, in LlamaModel_fast_forward\n    layer_outputs = torch.utils.checkpoint.checkpoint(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_compile.py\", line 24, in inner\n    return torch._dynamo.disable(fn, recursive)(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 328, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/external_utils.py\", line 17, in inner\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 451, in checkpoint\n    return CheckpointFunction.apply(function, preserve, *args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py\", line 539, in apply\n    return super().apply(*args, **kwargs)  # type: ignore[misc]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 230, in forward\n    outputs = run_function(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 578, in custom_forward\n    return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/gemma.py\", line 97, in GemmaDecoderLayer_fast_forward\n    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/hooks.py\", line 166, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\", line 318, in LlamaAttention_fast_forward\n    A = xformers_attention(Q, K, V, attn_bias = causal_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 223, in memory_efficient_attention\n    return _memory_efficient_attention(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 321, in _memory_efficient_attention\n    return _memory_efficient_attention_forward(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py\", line 337, in _memory_efficient_attention_forward\n    op = _dispatch_fw(inp, False)\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py\", line 120, in _dispatch_fw\n    return _run_priority_list(\n  File \"/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py\", line 63, in _run_priority_list\n    raise NotImplementedError(msg)\nNotImplementedError: No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(2, 4096, 16, 256) (torch.float16)\n     key         : shape=(2, 4096, 16, 256) (torch.float16)\n     value       : shape=(2, 4096, 16, 256) (torch.float16)\n     attn_bias   : <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n     p           : 0.0\n`decoderF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 128\n    xFormers wasn't build with CUDA support\n    attn_bias type is <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n    operator wasn't built - see `python -m xformers.info` for more info\n`flshattF@0.0.0` is not supported because:\n    xFormers wasn't build with CUDA support\n    requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)\n    operator wasn't built - see `python -m xformers.info` for more info\n`tritonflashattF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 128\n    xFormers wasn't build with CUDA support\n    requires device with capability > (8, 0) but your GPU has capability (7, 5) (too old)\n    operator wasn't built - see `python -m xformers.info` for more info\n    triton is not available\n    requires GPU with sm80 minimum compute capacity, e.g., A100/H100/L4\n    Only work on pre-MLIR triton for now\n`cutlassF` is not supported because:\n    xFormers wasn't build with CUDA support\n    operator wasn't built - see `python -m xformers.info` for more info\n`smallkF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 32\n    xFormers wasn't build with CUDA support\n    dtype=torch.float16 (supported: {torch.float32})\n    attn_bias type is <class 'xformers.ops.fmha.attn_bias.LowerTriangularMask'>\n    operator wasn't built - see `python -m xformers.info` for more info\n    unsupported embed per head: 256\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}